apiVersion: v1
kind: ConfigMap
metadata:
  name: clickhouse-backup-config
data:
  config.yml: | 
    general:
      remote_storage: s3           # REMOTE_STORAGE, if `none` then `upload` and  `download` command will fail
      max_file_size: 1073741824      # MAX_FILE_SIZE, 1G by default, useless when upload_by_part is true, use for split data parts files by archives  
      disable_progress_bar: true     # DISABLE_PROGRESS_BAR, show progress bar during upload and download, have sense only when `upload_concurrency` and `download_concurrency` equal 1
      backups_to_keep_local: 3       # BACKUPS_TO_KEEP_LOCAL, how much newest local backup should keep, 0 mean all created backups will keep on local disk
                                     # you shall to run `clickhouse-backup delete local <backup_name>` command to avoid useless disk space allocations
      backups_to_keep_remote: 21      # BACKUPS_TO_KEEP_REMOTE, how much newest backup should keep on remote storage, 0 mean all uploaded backups will keep on remote storage. 
                                     # if old backup is required for newer incremental backup, then it will don't delete. Be careful with long incremental backup sequences.
      log_level: info                # LOG_LEVEL, allow `debug`, `info`, `warn`, `error`
      allow_empty_backups: false     # ALLOW_EMPTY_BACKUPS
      # concurrency means parallel tables and parallel parts inside tables
      # for example 4 means max 4 parallel tables and 4 parallel parts inside one table, so equal 16 streams
      download_concurrency: 1        # DOWNLOAD_CONCURRENCY, max 255, default value round(sqrt(AVAILABLE_CPU_CORES / 2))  
      upload_concurrency: 1          # UPLOAD_CONCURRENCY, max 255, default value round(sqrt(AVAILABLE_CPU_CORES / 2))

      # RESTORE_SCHEMA_ON_CLUSTER, execute all schema related SQL queries with `ON CLUSTER` clause as Distributed DDL, 
      # look to `system.clusters` table for proper cluster name, also system.macros could be used, 
      # don't applicable when `use_embedded_backup_restore: true`
      restore_schema_on_cluster: ""   
      upload_by_part: true           # UPLOAD_BY_PART
      download_by_part: true         # DOWNLOAD_BY_PART
      use_resumable_state: false     # USE_RESUMABLE_STATE, allow resume upload and download according to <backup_name>.resumable file

      # RESTORE_DATABASE_MAPPING, restore rules from backup databases to target databases, which is useful on change destination database all atomic tables will create with new uuid.
      # for environment use following format "src_db1:target_db1,src_db2:target_db2" 
      restore_database_mapping: {}   
      retries_on_failure: 3          # RETRIES_ON_FAILURE, retry if failure during upload or download
      retries_pause: 30s             # RETRIES_PAUSE, time duration pause after each download or upload fail 
    clickhouse:
      username: default                # CLICKHOUSE_USERNAME
      password: ""                     # CLICKHOUSE_PASSWORD
      host: localhost                  # CLICKHOUSE_HOST
      port: 9000                       # CLICKHOUSE_PORT, don't use 8123, clickhouse-backup doesn't support HTTP protocol
      # CLICKHOUSE_DISK_MAPPING, use it if your system.disks on restored servers not the same with system.disks on server where backup was created
      # for environment use following format "disk_name1:disk_path1,disk_name2:disk_path2" 
      disk_mapping: {}
      # CLICKHOUSE_SKIP_TABLES, during create and restore tables which matched with these patterns will ignore
      # for environment use following format "pattern1,pattern2,pattern3" 
      skip_tables:                     
        - system.*
        - INFORMATION_SCHEMA.*
        - information_schema.*
      timeout: 5m                  # CLICKHOUSE_TIMEOUT
      freeze_by_part: false        # CLICKHOUSE_FREEZE_BY_PART, allows freeze part by part instead of freeze the whole table
      freeze_by_part_where: ""     # CLICKHOUSE_FREEZE_BY_PART_WHERE, allows parts filtering during freeze when freeze_by_part: true
      secure: false                # CLICKHOUSE_SECURE, use SSL encryption for connect
      skip_verify: false           # CLICKHOUSE_SKIP_VERIFY
      sync_replicated_tables: true # CLICKHOUSE_SYNC_REPLICATED_TABLES
      tls_key: ""                  # CLICKHOUSE_TLS_KEY, filename with TLS key file 
      tls_cert: ""                 # CLICKHOUSE_TLS_CERT, filename with TLS certificate file 
      tls_ca: ""                   # CLICKHOUSE_TLS_CA, filename with TLS custom authority file 
      log_sql_queries: true        # CLICKHOUSE_LOG_SQL_QUERIES, enable log clickhouse-backup SQL queries on `system.query_log` table inside clickhouse-server
      debug: false                 # CLICKHOUSE_DEBUG
      config_dir:      "/etc/clickhouse-server"              # CLICKHOUSE_CONFIG_DIR
      restart_command: "systemctl restart clickhouse-server" # CLICKHOUSE_RESTART_COMMAND, this command use when you try to restore with --rbac or --config options
      ignore_not_exists_error_during_freeze: true # CLICKHOUSE_IGNORE_NOT_EXISTS_ERROR_DURING_FREEZE, allow avoiding backup failures when you often CREATE / DROP tables and databases during backup creation, clickhouse-backup will ignore `code: 60` and `code: 81` errors during execute `ALTER TABLE ... FREEZE`
      check_replicas_before_attach: true # CLICKHOUSE_CHECK_REPLICAS_BEFORE_ATTACH, allow to avoid concurrent ATTACH PART execution when restore ReplicatedMergeTree tables
      use_embedded_backup_restore: false # CLICKHOUSE_USE_EMBEDDED_BACKUP_RESTORE, use BACKUP / RESTORE SQL statements instead of use regular SQL queries to allow compatible for modern ClickHouse server versions
    azblob:
      endpoint_suffix: "core.windows.net" # AZBLOB_ENDPOINT_SUFFIX
      account_name: ""             # AZBLOB_ACCOUNT_NAME
      account_key: ""              # AZBLOB_ACCOUNT_KEY
      sas: ""                      # AZBLOB_SAS
      use_managed_identity: false  # AZBLOB_USE_MANAGED_IDENTITY
      container: ""                # AZBLOB_CONTAINER
      path: ""                     # AZBLOB_PATH
      compression_level: 1         # AZBLOB_COMPRESSION_LEVEL
      compression_format: tar      # AZBLOB_COMPRESSION_FORMAT
      sse_key: ""                  # AZBLOB_SSE_KEY
      buffer_size: 0               # AZBLOB_BUFFER_SIZE, if less or eq 0 then calculated as max_file_size / max_parts_count, between 2Mb and 4Mb
      max_parts_count: 10000       # AZBLOB_MAX_PARTS_COUNT, number of parts for AZBLOB uploads, for properly calculate buffer size
      max_buffers: 3               # AZBLOB_MAX_BUFFERS
    s3:
      access_key: ""                   # S3_ACCESS_KEY
      secret_key: ""                   # S3_SECRET_KEY
      bucket: "sct-backups"                       # S3_BUCKET
      endpoint: "eu-central-1.linodeobjects.com"                     # S3_ENDPOINT
      region: eu-central-1                # S3_REGION
      acl: private                     # S3_ACL
      assume_role_arn: ""              # S3_ASSUME_ROLE_ARN
      force_path_style: false          # S3_FORCE_PATH_STYLE
      path: "DB/CLICKHOUSE/"                         # S3_PATH
      disable_ssl: false               # S3_DISABLE_SSL
      compression_level: 1             # S3_COMPRESSION_LEVEL
      compression_format: tar          # S3_COMPRESSION_FORMAT
      sse: ""                          # S3_SSE, empty (default), AES256, or aws:kms
      disable_cert_verification: false # S3_DISABLE_CERT_VERIFICATION
      use_custom_storage_class: false  # S3_USE_CUSTOM_STORAGE_CLASS
      storage_class: STANDARD          # S3_STORAGE_CLASS
      concurrency: 1                   # S3_CONCURRENCY
      part_size: 0                     # S3_PART_SIZE, if less or eq 0 then calculated as max_file_size / max_parts_count, between 5MB and 5Gb
      max_parts_count: 10000           # S3_MAX_PARTS_COUNT, number of parts for S3 multipart uploads
      allow_multipart_download: false  # S3_ALLOW_MULTIPART_DOWNLOAD, allow us fast download speed (same as upload), but will require additional disk space, download_concurrency * part size in worst case

      # S3_OBJECT_LABELS, allow setup metadata for each object during upload, use {macro_name} from system.macros and {backupName} for current backup name
      # for environment use following format "key1:value1,key2:value2" 
      object_labels: {}                
      debug: false                     # S3_DEBUG
    gcs:
      credentials_file: ""         # GCS_CREDENTIALS_FILE
      credentials_json: ""         # GCS_CREDENTIALS_JSON
      credentials_json_encoded: "" # GCS_CREDENTIALS_JSON_ENCODED
      bucket: ""                   # GCS_BUCKET
      path: ""                     # GCS_PATH
      compression_level: 1         # GCS_COMPRESSION_LEVEL
      compression_format: tar      # GCS_COMPRESSION_FORMAT
      storage_class: STANDARD      # GCS_STORAGE_CLASS

      # GCS_OBJECT_LABELS, allow setup metadata for each object during upload, use {macro_name} from system.macros and {backupName} for current backup name
      # for environment use following format "key1:value1,key2:value2" 
      object_labels: {}            
      debug: false                 # GCS_DEBUG
    cos:
      url: ""                      # COS_URL
      timeout: 2m                  # COS_TIMEOUT
      secret_id: ""                # COS_SECRET_ID
      secret_key: ""               # COS_SECRET_KEY
      path: ""                     # COS_PATH
      compression_format: tar      # COS_COMPRESSION_FORMAT
      compression_level: 1         # COS_COMPRESSION_LEVEL
    ftp:
      address: ""                  # FTP_ADDRESS
      timeout: 2m                  # FTP_TIMEOUT
      username: ""                 # FTP_USERNAME
      password: ""                 # FTP_PASSWORD
      tls: false                   # FTP_TLS
      path: ""                     # FTP_PATH
      compression_format: tar      # FTP_COMPRESSION_FORMAT
      compression_level: 1         # FTP_COMPRESSION_LEVEL
      debug: false                 # FTP_DEBUG
    sftp:
      address: ""                  # SFTP_ADDRESS
      username: ""                 # SFTP_USERNAME
      password: ""                 # SFTP_PASSWORD
      key: ""                      # SFTP_KEY
      path: ""                     # SFTP_PATH
      concurrency: 1               # SFTP_CONCURRENCY     
      compression_format: tar      # SFTP_COMPRESSION_FORMAT
      compression_level: 1         # SFTP_COMPRESSION_LEVEL
      debug: false                 # SFTP_DEBUG
    custom:  
      upload_command: ""           # CUSTOM_UPLOAD_COMMAND
      download_command: ""         # CUSTOM_DOWNLOAD_COMMAND
      delete_command: ""           # CUSTOM_DELETE_COMMAND
      list_command: ""             # CUSTOM_LIST_COMMAND
      command_timeout: "4h"          # CUSTOM_COMMAND_TIMEOUT
    api:
      listen: "localhost:7171"     # API_LISTEN
      enable_metrics: true         # API_ENABLE_METRICS
      enable_pprof: false          # API_ENABLE_PPROF
      username: ""                 # API_USERNAME, basic authorization for API endpoint
      password: ""                 # API_PASSWORD
      secure: false                # API_SECURE, use TLS for listen API socket
      certificate_file: ""         # API_CERTIFICATE_FILE
      private_key_file: ""         # API_PRIVATE_KEY_FILE
      create_integration_tables: false # API_CREATE_INTEGRATION_TABLES
      integration_tables_host: "" # API_INTEGRATION_TABLES_HOST, allow use DNS name to connect in `system.backup_list` and `system.backup_actions`
      allow_parallel: false        # API_ALLOW_PARALLEL, could allocate much memory and spawn go-routines, don't enable it if you not sure